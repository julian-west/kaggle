{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+3\">Inference Notebook</font>\n",
    "\n",
    "Explore simple ML models, different text embeddings, hyper parameter tuning and ensemble models.\n",
    "\n",
    "The BERT model got an accuracy of about 0.83 without any feature engineering. It will be interesting to see if traditional ML models can be tuned to perform as well/better than BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.so) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libgomp.so for UNIX-like OSes)\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['libgomp.so.1: cannot open shared object file: No such file or directory', 'libgomp.so.1: cannot open shared object file: No such file or directory']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-69ce20a2bbce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m                   \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;34m'libgomp.so for UNIX-like OSes)\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;34m'  * You are running 32-bit Python on a 64-bit OS\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             'Error message(s): {}\\n'.format(os_error_list))\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_log_callback_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.so) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libgomp.so for UNIX-like OSes)\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['libgomp.so.1: cannot open shared object file: No such file or directory', 'libgomp.so.1: cannot open shared object file: No such file or directory']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, log_loss\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in train_df.columns if col not in ['target','id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_df[features], train_df['target'], test_size=0.1,\n",
    "                                                 shuffle=True, random_state = 42)\n",
    "\n",
    "xtest = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "As per the competition rules. We will optimise for the F1_Score and also use the log_loss to evaluate each algorithms performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "As we will evalute different levels of text preprocessing, the processing steps will be kept in a seperate file so they can easily be added to as we evaluate each model. This will enable us to add different level of preprocessing to the pipelines.\n",
    "\n",
    "See `TweetProcessor` in `text_preprocessing.py` for text cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from text_preprocessing import TweetProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TweetProcessor()\n",
    "xtrain['text'] = tp.process_text(xtrain['text'])\n",
    "xvalid['text'] = tp.process_text(xvalid['text'])\n",
    "xtest['text'] = tp.process_text(xtest['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ML Models\n",
    "\n",
    "To begin with we will just evaluate the `text` column and create features from that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "There are a number of different methods for generating numerical features from text. We can try the following:\n",
    "- TF-IDF\n",
    "- CountVectorizer\n",
    "\n",
    "We will create features from using each method and evaluate which works better on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF embeddings\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Count Vectorizer\n",
    "ctv = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The first model to try is a simple logistic regression. We will use sklearn pipelines and gridsearch to find the best possible logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary to store results of each model\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard logistic regresssion\n",
    "lr_pipe_tfv = Pipeline([('tfv',tfv),\n",
    "                        ('lr',LogisticRegression())])\n",
    "\n",
    "lr_pipe_ctv = Pipeline([('ctv',ctv),\n",
    "                        ('lr',LogisticRegression())])\n",
    "\n",
    "#gridsearch parameters\n",
    "lr_param_grid = {'lr__C': [0.1, 1.0, 10],\n",
    "                'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelines with svd and scaling\n",
    "lr_pipe_tfv_svd = Pipeline([('tfv',tfv),\n",
    "                        ('svd',TruncatedSVD()),\n",
    "                        ('scl',StandardScaler()),\n",
    "                        ('lr',LogisticRegression())])\n",
    "\n",
    "lr_pipe_ctv_svd = Pipeline([('ctv',ctv),\n",
    "                        ('svd',TruncatedSVD()),\n",
    "                        ('scl',StandardScaler()),\n",
    "                        ('lr',LogisticRegression())])\n",
    "\n",
    "lr_svd_param_grid = {'svd__n_components' : [120, 180],\n",
    "                 'lr__C': [0.1, 1.0, 10], \n",
    "                 'lr__penalty': ['l1', 'l2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs_tfv = GridSearchCV(lr_pipe_tfv, param_grid=lr_param_grid, scoring='f1',\n",
    "                     verbose=5, n_jobs=-1, refit=True, cv=4)\n",
    "\n",
    "lr_gs_ctv = GridSearchCV(lr_pipe_ctv, param_grid=lr_param_grid, scoring='f1',\n",
    "                     verbose=5, n_jobs=-1, refit=True, cv=4)\n",
    "\n",
    "lr_svd_gs_tfv = GridSearchCV(lr_pipe_tfv_svd, param_grid=lr_svd_param_grid, scoring='f1',\n",
    "                     verbose=5, n_jobs=-1, refit=True, cv=4)\n",
    "\n",
    "lr_svd_gs_ctv = GridSearchCV(lr_pipe_ctv_svd, param_grid=lr_svd_param_grid, scoring='f1',\n",
    "                     verbose=5, n_jobs=-1, refit=True, cv=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: lr-tfv\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    7.2s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: lr-ctv\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    2.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: lr-svd-tfv\n",
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   26.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: lr-svd-ctv\n",
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyper-parameter tuning done\n"
     ]
    }
   ],
   "source": [
    "models = [lr_gs_tfv, lr_gs_ctv, lr_svd_gs_tfv, lr_svd_gs_ctv]\n",
    "names = ['lr-tfv','lr-ctv','lr-svd-tfv','lr-svd-ctv']\n",
    "\n",
    "for model, name in zip(models,names):\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    model.fit(xtrain['text'],ytrain)\n",
    "    \n",
    "print(\"\\nHyper-parameter tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr-tfv: \n",
      "\tBest GridSearch score:  0.7433\n",
      "\tScore on validation set:  0.7227\n",
      "lr-ctv: \n",
      "\tBest GridSearch score:  0.7450\n",
      "\tScore on validation set:  0.7212\n",
      "lr-svd-tfv: \n",
      "\tBest GridSearch score:  0.7265\n",
      "\tScore on validation set:  0.7072\n",
      "lr-svd-ctv: \n",
      "\tBest GridSearch score:  0.7061\n",
      "\tScore on validation set:  0.6912\n"
     ]
    }
   ],
   "source": [
    "def evaluate_gridsearch_models(models,names,xvalid):\n",
    "    val_scores = {}\n",
    "    for model, name in zip(models, names):\n",
    "        print(f\"{name}: \\n\\tBest GridSearch score: {model.best_score_: 0.4f}\")\n",
    "        predictions = model.predict(xvalid)\n",
    "        val_score = f1_score(yvalid,predictions)\n",
    "        print(f\"\\tScore on validation set: {val_score: 0.4f}\")\n",
    "        val_scores[name] = val_score\n",
    "            \n",
    "    return val_scores\n",
    "    \n",
    "\n",
    "lr_val_scores = evaluate_gridsearch_models(models,names,xvalid['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model with best performance on the unseen validation set is the best Logistic Regression model with CountVectorizer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record best model into dictionary for comparison with other models later\n",
    "model_results['lr-ctv'] = {'model': lr_gs_ctv, 'score': lr_val_scores['lr-ctv']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification report for best logistic regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.499 \n",
      "F1 score: 0.721 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       426\n",
      "           1       0.78      0.67      0.72       336\n",
      "\n",
      "    accuracy                           0.77       762\n",
      "   macro avg       0.77      0.76      0.76       762\n",
      "weighted avg       0.77      0.77      0.77       762\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_classification_report(model):\n",
    "    \"\"\"Print classification report and confusion matrix\"\"\"\n",
    "    predictions_prob = model.predict_proba(xvalid['text'])\n",
    "    predictions_classes = model.predict(xvalid['text'])\n",
    "\n",
    "    print (\"Log loss: %0.3f \" % log_loss(yvalid, predictions_prob))\n",
    "    print(\"F1 score: %0.3f \" % f1_score(yvalid, predictions_classes))\n",
    "    print(classification_report(yvalid, predictions_classes))\n",
    "\n",
    "    sns.heatmap(confusion_matrix(yvalid,predictions_classes),annot=True,fmt='.0f',cmap='Blues', cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "create_classification_report(lr_gs_ctv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pipeline_tfv = Pipeline([('tfv',tfv),\n",
    "                      ('nb',nb)])\n",
    "\n",
    "nb_pipeline_ctv = Pipeline([('ctv',ctv),\n",
    "                      ('nb',nb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gs_tfv = GridSearchCV(nb_pipeline_tfv, param_grid=nb_param_grid, scoring='f1',\n",
    "                     verbose=5, n_jobs=-1, refit=True, cv=4)\n",
    "\n",
    "nb_gs_ctv = GridSearchCV(nb_pipeline_ctv, param_grid=nb_param_grid, scoring='f1',\n",
    "                     verbose=5, n_jobs=-1, refit=True, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: nb-tfv\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    5.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model: nb-ctv\n",
      "Fitting 4 folds for each of 6 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:    1.7s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter tuning done\n"
     ]
    }
   ],
   "source": [
    "models = [nb_gs_tfv, nb_gs_ctv]\n",
    "names = ['nb-tfv','nb-ctv']\n",
    "\n",
    "for model, name in zip(models,names):\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    model.fit(xtrain['text'],ytrain)\n",
    "\n",
    "print(\"Hyper-parameter tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb-tfv: \n",
      "\tBest GridSearch score:  0.7322\n",
      "\tScore on validation set:  0.7092\n",
      "nb-ctv: \n",
      "\tBest GridSearch score:  0.7512\n",
      "\tScore on validation set:  0.7339\n"
     ]
    }
   ],
   "source": [
    "nb_val_scores = evaluate_gridsearch_models(models,names,xvalid['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.763 \n",
      "F1 score: 0.734 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.81       426\n",
      "           1       0.78      0.69      0.73       336\n",
      "\n",
      "    accuracy                           0.78       762\n",
      "   macro avg       0.78      0.77      0.77       762\n",
      "weighted avg       0.78      0.78      0.78       762\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_classification_report(nb_gs_ctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record best model into dictionary for comparison with other models later\n",
    "model_results['nb-ctv'] = {'model': nb_gs_ctv, 'score': nb_val_scores['nb-ctv']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove vectors is another method for generating features from text.\n",
    "\n",
    "Note that the text file containing the embeddings can be downloaded from `http://www-nlp.stanford.edu/data/glove.840B.300d.zip` - it is not included in this git repository due to its large size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a50b57ae58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcoefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index = {}\n",
    "with open('../external/glove.840B.300d/glove.840B.300d/glove.840B.300d.txt') as f:\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Finally, we will try an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipe_tfv = Pipeline([('tfv',tfv),\n",
    "                         ('xgb',xgb_clf)\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "xgb_pipe_ctv = Pipeline([('ctv',ctv),\n",
    "                         ('xgb',xgb_clf)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\"xgb__n_estimators\": np.arange(400, 10000, 1000), 'xgb__max_depth': range(1, 20,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_tfv = GridSearchCV(xgb_pipe_tfv, param_grid=xgb_param_grid, scoring='f1',\n",
    "                     verbose=10, n_jobs=-1, refit=True, cv=4)\n",
    "\n",
    "xgb_gs_ctv = GridSearchCV(xgb_pipe_ctv, param_grid=xgb_param_grid, scoring='f1',\n",
    "                     verbose=10, n_jobs=-1, refit=True, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_gs_tfv.fit(xtrain['text'],ytrain)\n",
    "xgb_gs_ctv.fit(xtrain['text'],ytrain)\n",
    "\n",
    "print(\"Hyperparameter tuning complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classification_report(xgb_gs_tfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_csv(model,model_name,processed_test_data):\n",
    "    \"\"\"Make preidctions on test set and save to dataframe\n",
    "    \n",
    "    Args:\n",
    "        model: grid search model\n",
    "        model_name (str): name of model to include in the file name of saved model\n",
    "        processed_test_data(df): preprocessed test data\n",
    "\n",
    "    \"\"\"\n",
    "    preds = model.predict(processed_test_data)\n",
    "    final_predictions_df = pd.DataFrame({'id':test_df['id'],'target':preds})\n",
    "    final_predictions_df.to_csv(f\"./submissions/{model_name}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission_csv(xgb_gs_ctv,'xgb_tfv',xtest['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "Many ideas for this notebook can from this [notebook](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) from Kaggle Grandmaster Abhishek Thakur. I would recommend checking out more of his [content](https://www.youtube.com/user/abhisheksvnit), he has great tutorials particularly on using BERT with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
