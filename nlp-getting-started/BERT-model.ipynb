{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOrOvYtNDrcEA4W2ELSz7rw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julian-west/kaggle/blob/master/nlp-getting-started/BERT-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAv3DlJTPVDo",
        "colab_type": "text"
      },
      "source": [
        "# BERT Classification Model\n",
        "\n",
        "Simple BERT model for the [Kaggle Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview) competition.\n",
        "\n",
        "This notebook achieves a score of about 0.81."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRlCTxzL7-a3",
        "colab_type": "text"
      },
      "source": [
        "# Standard Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIsFqa028Gs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR8gF03P7VKy",
        "colab_type": "text"
      },
      "source": [
        "# Load Datafiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2EsA1ja7Yqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "519a4eda-763b-4f28-f696-f988566f18f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsQNO6g177_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_LOCATION = \"/content/drive/My Drive/kaggle/nlp-getting-started/\"\n",
        "\n",
        "train_df = pd.read_csv(DATA_LOCATION + \"train.csv\")\n",
        "test_df = pd.read_csv(DATA_LOCATION + \"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0WvS7528pbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "4a997f43-5129-489e-da1c-6b7fdac0a956"
      },
      "source": [
        "display(train_df.head())\n",
        "display(test_df.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpnQo6RJ7ZDq",
        "colab_type": "text"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYEKulTK82NF",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fK1-dl8PgLd",
        "colab_type": "code",
        "outputId": "8bbe5602-b693-483f-b0f0-8fa0d7081c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PudLjO2JPihe",
        "colab_type": "code",
        "outputId": "c1dbad6a-3a20-4087-8e34-5d53f813e6c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification \n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlYIVaWg8-QG",
        "colab_type": "text"
      },
      "source": [
        "## LoadDataset Class\n",
        "\n",
        "Class to load and preprocess the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zimuU7brP3ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetProcessor:\n",
        "\n",
        "  def __init__(self, tweets, labels=[], maxlen=64):\n",
        "    self.tweets = tweets\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    self.maxlen = maxlen\n",
        "    self.labels = labels\n",
        "\n",
        "\n",
        "  def process_text(self):\n",
        "    \"\"\"processes tweets and convert to tensors for BERT model\"\"\"\n",
        "  \n",
        "    #remove html\n",
        "    self.tweets = self.tweets.apply(\n",
        "        lambda x: re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\",\"\",x))\n",
        "    \n",
        "    #convert to BERT encoded IDs\n",
        "    input_ids = [self.tokenizer.encode(\n",
        "        t, add_special_tokens = True) for t in self.tweets]\n",
        "\n",
        "    #padd sequences\n",
        "    input_ids = pad_sequences(input_ids, maxlen=self.maxlen,\n",
        "                              dtype='long', value=0, \n",
        "                              truncating='post', padding='post'\n",
        "                              )\n",
        "\n",
        "    #create masks for each tweet (1 if token >0 else 0)\n",
        "    att_masks = [[int(token_id > 0) for token_id in input_id\n",
        "                       ] for input_id in input_ids]\n",
        "\n",
        "    input_ids_tensors = torch.tensor(input_ids)\n",
        "    att_masks_tensors = torch.tensor(att_masks)\n",
        "\n",
        "\n",
        "    if len(self.labels) == len(self.tweets):\n",
        "      labels_tensors = torch.tensor(self.labels.ravel())\n",
        "      return input_ids_tensors, att_masks_tensors, labels_tensors\n",
        "    else:\n",
        "      return input_ids_tensors, att_masks_tensors\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oWrXoc7uDAz",
        "colab_type": "text"
      },
      "source": [
        "## Training validation split\n",
        "\n",
        "Split training data into train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00yK0c5gt6C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "TEST_SIZE = 0.1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data, val_data = train_test_split(\n",
        "    train_df,\n",
        "    random_state = SEED,\n",
        "    test_size = 0.1,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_inputs, train_masks, train_labels = TweetProcessor(\n",
        "    train_data['text'], \n",
        "    labels=train_data['target']).process_text()\n",
        "\n",
        "val_inputs, val_masks, val_labels = TweetProcessor(\n",
        "    val_data['text'], \n",
        "    labels=val_data['target']).process_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqdH-5XiwLuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, \n",
        "                              sampler = train_sampler)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                            sampler=val_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBfV2N5gqsyZ",
        "colab_type": "text"
      },
      "source": [
        "## Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrGatsPv5tZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#number of traning epochs\n",
        "EPOCHS = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPeBqE1Aqxh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    )\n",
        "\n",
        "\n",
        "#Optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8\n",
        "                  )\n",
        "\n",
        "#create the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = len(train_dataloader) * EPOCHS\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pW-cs4dQyCIb"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZfn4Z4dyDTR",
        "colab_type": "text"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mNe7ECEyFxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "  pred_flat = np.argmax(preds,axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat)/len(labels_flat)\n",
        "\n",
        "\n",
        "#format the time \n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round((elapsed)))\n",
        "  return str(datetime.timedelta(seconds = elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n--giASpyGSM",
        "colab_type": "text"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykbS5OYrxSdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24b3aab8-50bb-40d0-d197-069220e0696a"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "print(f\"Using {device}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bM-Fg_ixpSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKvvVzFJyjXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_model(nnet, val_dataloader, device):\n",
        "  \"\"\"Evaluate current model on validation set\"\"\"\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  nnet.eval()\n",
        "\n",
        "  loss, accuracy = 0, 0\n",
        "  \n",
        "  eval_steps = 0\n",
        "  for batch in val_dataloader:\n",
        "    b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "    with torch.no_grad():\n",
        "      #get predictions \n",
        "      outputs = nnet(b_input_ids, \n",
        "                     token_type_ids = None,\n",
        "                     attention_mask = b_input_mask)\n",
        "\n",
        "    #calc loss\n",
        "    logits = outputs[0].detach().cpu().numpy()\n",
        "    labels = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_accuracy = flat_accuracy(logits, labels)\n",
        "    \n",
        "    accuracy += tmp_accuracy\n",
        "\n",
        "    eval_steps += 1\n",
        "\n",
        "  print(f\"----Accuracy: {accuracy/eval_steps}\")\n",
        "  print(f\"----Validation took {format_time(time.time() - t0)}\")\n",
        "\n",
        "  return accuracy/eval_steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvl7qcPR1sLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(nnet, optimizer, train_dataloader, val_dataloader, device, epochs=EPOCHS):\n",
        "\n",
        "  nnet.to(device)\n",
        "  nnet.train()\n",
        "\n",
        "  #store loss values to plot after training\n",
        "  loss_values = []\n",
        "  val_loss = []\n",
        "  val_accuracy = []\n",
        "\n",
        "  print(\"=============== STARTING TRAINING ===============\")\n",
        "  t0 = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    print(f\"\\n\\n======= EPOCH {epoch+1} =======\")\n",
        "    t1 = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "      if step % 40 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t1)\n",
        "        print(f\"    Batch {step} of {len(train_dataloader)}. Elapsed time {elapsed}\")\n",
        "\n",
        "\n",
        "      #move inputs to device\n",
        "      b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "      #zero gradients\n",
        "      nnet.zero_grad()\n",
        "\n",
        "      #perform forward pass\n",
        "      outputs = nnet(\n",
        "          b_input_ids,\n",
        "          token_type_ids = None,\n",
        "          attention_mask = b_input_mask,\n",
        "          labels = b_labels\n",
        "          )\n",
        "      \n",
        "      loss = outputs[0]\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      #back propagate the gradients\n",
        "      loss.backward()\n",
        "\n",
        "      #clip gradients to tackle exploding gradients problem\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "\n",
        "      #update parameters and take a step using the computed gradient\n",
        "      optimizer.step()\n",
        "\n",
        "      #update learning rate\n",
        "      scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss/len(train_dataloader)\n",
        "\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"   Average training loss: {avg_train_loss:.2f}\")\n",
        "    print(f\"   Training epoch took: {format_time(time.time() - t1)}\")\n",
        "\n",
        "    print(f\"\\n\\n======= Running Validation =======\")\n",
        "    val_accuracy = validate_model(nnet,val_dataloader,device)\n",
        "\n",
        "\n",
        "  print(\"\\n\\n\\n TRAINING COMPLETED!\")\n",
        "  total_time = format_time(time.time() - t0)\n",
        "  print(f\"------ Training took {total_time}\")\n",
        "\n",
        "  return loss_values, val_accuracy\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h97g5S7z2MwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bf0ceee-f9fe-49a7-9f72-058b4b34e982"
      },
      "source": [
        "loss_values, val_accuracy = train(model, optimizer, train_dataloader, val_dataloader, device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=============== STARTING TRAINING ===============\n",
            "\n",
            "\n",
            "======= EPOCH 1 =======\n",
            "    Batch 40 of 215. Elapsed time 0:00:19\n",
            "    Batch 80 of 215. Elapsed time 0:00:38\n",
            "    Batch 120 of 215. Elapsed time 0:00:58\n",
            "    Batch 160 of 215. Elapsed time 0:01:17\n",
            "    Batch 200 of 215. Elapsed time 0:01:36\n",
            "\n",
            "   Average training loss: 0.46\n",
            "   Training epoch took: 0:01:43\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.8389423076923076\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "======= EPOCH 2 =======\n",
            "    Batch 40 of 215. Elapsed time 0:02:06\n",
            "    Batch 80 of 215. Elapsed time 0:02:25\n",
            "    Batch 120 of 215. Elapsed time 0:02:44\n",
            "    Batch 160 of 215. Elapsed time 0:03:03\n",
            "    Batch 200 of 215. Elapsed time 0:03:23\n",
            "\n",
            "   Average training loss: 0.32\n",
            "   Training epoch took: 0:01:43\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.827323717948718\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "======= EPOCH 3 =======\n",
            "    Batch 40 of 215. Elapsed time 0:03:52\n",
            "    Batch 80 of 215. Elapsed time 0:04:11\n",
            "    Batch 120 of 215. Elapsed time 0:04:30\n",
            "    Batch 160 of 215. Elapsed time 0:04:50\n",
            "    Batch 200 of 215. Elapsed time 0:05:09\n",
            "\n",
            "   Average training loss: 0.22\n",
            "   Training epoch took: 0:01:43\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.8221153846153846\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "======= EPOCH 4 =======\n",
            "    Batch 40 of 215. Elapsed time 0:05:38\n",
            "    Batch 80 of 215. Elapsed time 0:05:57\n",
            "    Batch 120 of 215. Elapsed time 0:06:16\n",
            "    Batch 160 of 215. Elapsed time 0:06:36\n",
            "    Batch 200 of 215. Elapsed time 0:06:55\n",
            "\n",
            "   Average training loss: 0.13\n",
            "   Training epoch took: 0:01:43\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.8121995192307692\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "======= EPOCH 5 =======\n",
            "    Batch 40 of 215. Elapsed time 0:07:24\n",
            "    Batch 80 of 215. Elapsed time 0:07:43\n",
            "    Batch 120 of 215. Elapsed time 0:08:03\n",
            "    Batch 160 of 215. Elapsed time 0:08:22\n",
            "    Batch 200 of 215. Elapsed time 0:08:41\n",
            "\n",
            "   Average training loss: 0.08\n",
            "   Training epoch took: 0:01:43\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.8080929487179488\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "======= EPOCH 6 =======\n",
            "    Batch 40 of 215. Elapsed time 0:09:10\n",
            "    Batch 80 of 215. Elapsed time 0:09:29\n",
            "    Batch 120 of 215. Elapsed time 0:09:49\n",
            "    Batch 160 of 215. Elapsed time 0:10:08\n",
            "    Batch 200 of 215. Elapsed time 0:10:27\n",
            "\n",
            "   Average training loss: 0.05\n",
            "   Training epoch took: 0:01:43\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.796073717948718\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "======= EPOCH 7 =======\n",
            "    Batch 40 of 215. Elapsed time 0:10:56\n",
            "    Batch 80 of 215. Elapsed time 0:11:15\n",
            "    Batch 120 of 215. Elapsed time 0:11:35\n",
            "    Batch 160 of 215. Elapsed time 0:11:54\n",
            "    Batch 200 of 215. Elapsed time 0:12:13\n",
            "\n",
            "   Average training loss: 0.03\n",
            "   Training epoch took: 0:01:42\n",
            "\n",
            "\n",
            "======= Running Validation =======\n",
            "----Accuracy: 0.807391826923077\n",
            "----Validation took 0:00:03\n",
            "\n",
            "\n",
            "\n",
            " TRAINING COMPLETED!\n",
            "------ Training took 0:12:23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqN8y_In583i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a88b1d4a-a517-4a64-a536-148bd7c998fa"
      },
      "source": [
        "#plot loss values\n",
        "plt.plot(range(1,EPOCHS+1),loss_values)\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.xticks([i for i in range(1,EPOCHS+1)])\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niXEqAhGRGKciEBGJcSoCEZEY9/9JBil+YcUFCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztvM7w0jSkt3",
        "colab_type": "text"
      },
      "source": [
        "# Make final predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QFPIOYBWuLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_inputs, test_masks = TweetProcessor(\n",
        "    test_df['text'], \n",
        "    labels=[]).process_text()\n",
        "\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax2fRogTXMak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a1b4f975-4780-469d-9dc6-3196b0f575e4"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for i, batch in enumerate(test_dataloader):\n",
        "  if i % 40 == 0:\n",
        "    print(f\"\\n Making predictions for batch {i} of {len(test_dataloader)}\")\n",
        "\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(b_input_ids, \n",
        "                    token_type_ids = None,\n",
        "                    attention_mask = b_input_mask)\n",
        "    \n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "\n",
        "  predictions.append(logits)\n",
        "\n",
        "print(\"\\n   DONE   \")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Making predictions for batch 0 of 102\n",
            "\n",
            " Making predictions for batch 40 of 102\n",
            "\n",
            " Making predictions for batch 80 of 102\n",
            "\n",
            "   DONE   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWFi1G99Xwin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#find argmax and flatten batches list\n",
        "final_predictions = [i for j in range(len(predictions)) for i in np.argmax(predictions[j],axis=1)]\n",
        "\n",
        "test_predictions = pd.DataFrame({'id':test_df['id'],'target':final_predictions})\n",
        "test_predictions.to_csv(\"submission2.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qtCk6quX3lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}